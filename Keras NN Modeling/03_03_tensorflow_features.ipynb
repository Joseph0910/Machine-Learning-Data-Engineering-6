{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-02-tensorflow-features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNE4fO9ZS28nuF/31wVV21w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/keras-tutorial-odsc2020/blob/master/03_03_tensorflow_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxpNemFDbbpm"
      },
      "source": [
        "__NOTE: remember to set your runtime type to GPU for this notebook.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYFblvDvaHgD"
      },
      "source": [
        "# Using underlying Tensorflow features\n",
        "\n",
        "In this session, we will look at some interesting Tensorflow features that you should know as a `tf.keras` developer. The \"old\" Keras used to provide a set of `keras.backend` functions that would delegate to the appropriate Tensorflow or Theano function.\n",
        "\n",
        "Because `tf.keras` has only a single backend, the entire Tensorflow library is at your disposal for customizing Keras. The old backend functions are still available via the `tf.keras.backend` package, but chances are very high that it maps 1:1 to an actual TF function.\n",
        "\n",
        "We will cover the following:\n",
        "* tf.data.Dataset\n",
        "* tf.GradientTape and @tf.function\n",
        "* Distributed training using tf.strategy\n",
        "\n",
        "Finally, we will cover some strategies for exploring further and keeping up-to-date on Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQrEjgknbfWl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EmDdwqEK5UO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import time\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQlH5MPQodEz"
      },
      "source": [
        "## tf.data.Dataset\n",
        "\n",
        "Tensorflow allows you to create iterators over your data, and Keras can consume these Dataset objects instead of Numpy matrices (such as `(Xtrain, ytrain)` pairs we were using so far).\n",
        "\n",
        "You can create Datasets from various things, [full list is here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), common ones are:\n",
        "\n",
        "* `Dataset.from_tensor_slices`\n",
        "* `Dataset.from_generator`\n",
        "* `tf.data.TFRecordDataset` objects ([see docs](https://www.tensorflow.org/guide/data#consuming_tfrecord_data)).\n",
        "\n",
        "Dataset is an iterator, so full dataset does not need to fit in memory.\n",
        "\n",
        "General pattern:\n",
        "* Create dataset from input data.\n",
        "* Apply transformations (shuffle, batch, etc) on the data.\n",
        "* Consume the data in streaming manner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEkKoS0a4FEf"
      },
      "source": [
        "### Dataset.from_tensor_slices()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn8AVq92ADba",
        "outputId": "9a5bb972-8d58-4531-ab54-665adda7f9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "(Xtrain, ytrain), _ = keras.datasets.mnist.load_data()\n",
        "\n",
        "Xtrain = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1).astype(np.float32)\n",
        "ytrain = keras.utils.to_categorical(ytrain, num_classes=10)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtrain, ytrain)).shuffle(len(Xtrain)).batch(32)\n",
        "\n",
        "for Xtrain_b, ytrain_b in train_ds:\n",
        "  print(Xtrain_b.shape, ytrain_b.shape)\n",
        "  break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(32, 28, 28, 1) (32, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcW-EPptA8M8"
      },
      "source": [
        "You can also apply various transformations to a dataset. For example, if we want to resize each of these images, we could do something as shown below.\n",
        "\n",
        "Other transformations are __filter__, __enumerate__, __flat_map__, etc.\n",
        "\n",
        "Note that the parameter to `lambda` should mirror the structure of the dataset element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJMy7-ZbGAtO",
        "outputId": "34d0e45a-7b6e-427b-9678-a241edc68959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def resize_image(x, y):\n",
        "  image = tf.image.resize(x, (32, 32))\n",
        "  return (image, y)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((Xtrain, ytrain)).batch(32)\n",
        "resized_ds = train_ds.map(lambda x, y: resize_image(x, y))\n",
        "for X, y in resized_ds:\n",
        "  print(X.shape, y.shape)\n",
        "  break\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 32, 1) (32, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzta-n83KsFR"
      },
      "source": [
        "### Dataset.from_generator()\n",
        "\n",
        "Another option could be to wrap the Keras `ImageDataGenerator` with a Dataset, so that way we have a data stream of augmented images.\n",
        "\n",
        "* Declare a `ImageDataGenerator`\n",
        "* Define an iterator using `ImageDataGenerator.flow`.\n",
        "* Create `Dataset` with `from_generator`, passing the iterator to the function via a lambda.\n",
        "\n",
        "You can call `repeat(n)` on the Dataset to get the benefit of real data augmentation, i.e. repeat(10) will given you 10x the data (original + random augmented according to your configuration of `ImageDataGenerator`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQGEx_fJK9Ui",
        "outputId": "0e52e76c-2e46-47eb-abf4-176678838b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "image_augmenter = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "data_generator = image_augmenter.flow(Xtrain, ytrain, batch_size=BATCH_SIZE)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator,\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([BATCH_SIZE, 28, 28, 1],\n",
        "                   [BATCH_SIZE, 10])\n",
        ")\n",
        "\n",
        "for X, y in train_ds:\n",
        "  print(X.shape, y.shape)\n",
        "  break\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 28, 28, 1) (32, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtHe56n3MAGV",
        "outputId": "95024fa2-ab4b-4599-9628-d0d55155b6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xtrain.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvH1Z1Tw1FWz"
      },
      "source": [
        "### Keras preprocessing tools\n",
        "\n",
        "In addition, Keras provides the following two convenience functions that can convert images and text files into datasets.\n",
        "\n",
        "* `preprocessing.text_dataset_from_directory` -- [see docs](https://keras.io/api/preprocessing/text/)\n",
        "* `preprocessing.image_dataset_from_directory` -- [see docs](https://keras.io/api/preprocessing/image/)\n",
        "\n",
        "Both depend on a standard directory structure that looks like this:\n",
        "\n",
        "```\n",
        "directory\n",
        "  |\n",
        "  +-- class_label_1\n",
        "  |    |\n",
        "  |    +-- item_1 (image or text, one record)\n",
        "  |    |\n",
        "  |    +-- ...\n",
        "  |    |\n",
        "  |    +-- item_n\n",
        "  +-- class_labels_2\n",
        "  |    |\n",
        "```\n",
        "\n",
        "In general, though, it is possible to create a Dataset from any generator function using `Dataset.from_generator()`, similar to how we built one from `ImageDataGenerator` (see [consuming Python generators](https://www.tensorflow.org/guide/data#consuming_python_generators)) for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTXDAG4tLM7e"
      },
      "source": [
        "## tf.GradientTape and @tf.function\n",
        "\n",
        "When you need more control over the training loop than calling `fit` on your Keras model, then you can use `tf.GradientTape` which allows for automatic differentiation.\n",
        "\n",
        "The `@tf.function` annotation is meant to put the training loop in \"compiled-mode\", i.e., in non-eager mode.\n",
        "\n",
        "A good example for when you might want more control over the training loop is with a Generative Adversarial Network (GAN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9jhrdrOPDem"
      },
      "source": [
        "### Generative Adversarial Network (GAN)\n",
        "\n",
        "A GAN consists of __two competing networks__ -- a generator and a discriminator - that are trained in an __adversarial manner__.\n",
        "\n",
        "Generator learns to create output that looks real, and discriminator learns to tell real from fake output.\n",
        "\n",
        "As training progresses, generator learns to create output that looks more and more real, and discriminator gets better at telling them apart.\n",
        "\n",
        "The training converges when there is no more improvement in the discriminator.\n",
        "\n",
        "The example here is adapted from the [Tensorflow tutorial on DCGAN](https://www.tensorflow.org/tutorials/generative/dcgan)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1V2Nv-KLJ1B",
        "outputId": "b89f8003-2e63-4c7c-ebf7-aaacb45a3a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# NOTE: we are not using labels here, at all\n",
        "(Xtrain, _), (_, _) = keras.datasets.mnist.load_data()\n",
        "\n",
        "Xtrain.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL1hd9kvXnvD"
      },
      "source": [
        "NOISE_DIM = 100\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 50"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Ovk4xRR78k"
      },
      "source": [
        "# reshape images to add channel and set type to float\n",
        "Xtrain = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# normalize pixel values to the range [-1, 1]\n",
        "Xtrain = (Xtrain - 127.5) / 127.5\n",
        "\n",
        "# convert to tf Dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(Xtrain).shuffle(len(Xtrain)).batch(BATCH_SIZE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCztiHC_VtvO"
      },
      "source": [
        "Both the generator and discriminator are convolutional models.\n",
        "\n",
        "The [Conv2DTranspose layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) is the inverse operation of Convolution, and is sometimes called Deconvolution. Instead of summarizing an area of the image into a smaller but deeper tensor, it expands a small deep tensor into a flatter larger one.\n",
        "\n",
        "So the generator takes a (batch of) random vectors and tries to produce a (batch of) images of shape (None, 28, 28, 1), here None is the batch dimension.\n",
        "\n",
        "The discriminator takes the generated image and tries to predict a number between 0 and 1, which gets converted to 0 (fake) or 1 (true) by the Binary Cross Entropy loss function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgVd0C3dj8oQ"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn5VhyizSQF0"
      },
      "source": [
        "def build_generator():\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(7*7*256, use_bias=False, input_shape=(NOISE_DIM,)))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(keras.layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(keras.layers.Conv2DTranspose(\n",
        "        128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(keras.layers.Conv2DTranspose(\n",
        "        64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "\n",
        "    model.add(keras.layers.Conv2DTranspose(\n",
        "        1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_discriminator():\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                            input_shape=[28, 28, 1]))\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.LeakyReLU())\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I66Rh4FFj_IS"
      },
      "source": [
        "### Loss functions and optimizers\n",
        "\n",
        "The discriminator loss quantifies how well discriminator is able to distinguish real from fake, so it is a combination of the difference between discriminator predictions on real images to an array of 1s, and discriminator predictions on fake (generated) images to an array of 0s.\n",
        "\n",
        "The generator loss quantifies how well it was able to trick the discriminator. So if generator is performing well, it should be able to trick the discriminator completely. So the loss is measured as the difference between the discriminator predictions against fake images generated by generator against an array of 1s.\n",
        "\n",
        "For both generator and discriminator, the optimizer we choose is Adam with learning rate 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lCMhwvRWXUn"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXqDArYkCbg"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Notice the following:\n",
        "\n",
        "* You have to specify `training=True` for the discriminator and generator. In (high level) Keras, this is set by Keras depending on whether you call `fit` or `predict` (or `evaluate`).\n",
        "* You have to compute the loss by applying the appropriate loss function to the output of the model.\n",
        "* You have to compute and apply the gradient of the loss back into the model using `optimizer.apply_gradients()`.\n",
        "\n",
        "The `train` function just calls the `train_step` function for the given number of epochs and writes out some progress information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1_HBzBwWx3H"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images, generator, discriminator):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(\n",
        "        zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyZ5xSf0XCMe"
      },
      "source": [
        "def train(dataset, generator, discriminator, num_epochs):\n",
        "  for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch, generator, discriminator)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ-H4r2YXRBP",
        "outputId": "5ba07fcc-e2ab-4646-febf-da6d8f2cae19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "train(train_ds, generator, discriminator, NUM_EPOCHS)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 18.464213848114014 sec\n",
            "Time for epoch 2 is 10.16083312034607 sec\n",
            "Time for epoch 3 is 10.236183643341064 sec\n",
            "Time for epoch 4 is 10.37952709197998 sec\n",
            "Time for epoch 5 is 10.544126033782959 sec\n",
            "Time for epoch 6 is 10.750604152679443 sec\n",
            "Time for epoch 7 is 11.033140420913696 sec\n",
            "Time for epoch 8 is 11.192206621170044 sec\n",
            "Time for epoch 9 is 10.918406963348389 sec\n",
            "Time for epoch 10 is 10.74901533126831 sec\n",
            "Time for epoch 11 is 10.673380613327026 sec\n",
            "Time for epoch 12 is 10.661564350128174 sec\n",
            "Time for epoch 13 is 10.71351671218872 sec\n",
            "Time for epoch 14 is 10.804184913635254 sec\n",
            "Time for epoch 15 is 10.84982442855835 sec\n",
            "Time for epoch 16 is 10.804355382919312 sec\n",
            "Time for epoch 17 is 10.773839712142944 sec\n",
            "Time for epoch 18 is 10.760342836380005 sec\n",
            "Time for epoch 19 is 10.737904787063599 sec\n",
            "Time for epoch 20 is 10.722037076950073 sec\n",
            "Time for epoch 21 is 10.737329006195068 sec\n",
            "Time for epoch 22 is 10.740576028823853 sec\n",
            "Time for epoch 23 is 10.757464170455933 sec\n",
            "Time for epoch 24 is 10.74527907371521 sec\n",
            "Time for epoch 25 is 10.750063180923462 sec\n",
            "Time for epoch 26 is 10.759083986282349 sec\n",
            "Time for epoch 27 is 10.750671863555908 sec\n",
            "Time for epoch 28 is 10.750675201416016 sec\n",
            "Time for epoch 29 is 10.754776000976562 sec\n",
            "Time for epoch 30 is 10.732877492904663 sec\n",
            "Time for epoch 31 is 10.739501953125 sec\n",
            "Time for epoch 32 is 10.745087146759033 sec\n",
            "Time for epoch 33 is 10.752689599990845 sec\n",
            "Time for epoch 34 is 10.748434782028198 sec\n",
            "Time for epoch 35 is 10.743379831314087 sec\n",
            "Time for epoch 36 is 10.744064807891846 sec\n",
            "Time for epoch 37 is 10.754608869552612 sec\n",
            "Time for epoch 38 is 10.747576713562012 sec\n",
            "Time for epoch 39 is 10.737396478652954 sec\n",
            "Time for epoch 40 is 10.748005867004395 sec\n",
            "Time for epoch 41 is 10.746915578842163 sec\n",
            "Time for epoch 42 is 10.77225136756897 sec\n",
            "Time for epoch 43 is 10.803822040557861 sec\n",
            "Time for epoch 44 is 10.825467824935913 sec\n",
            "Time for epoch 45 is 10.802480459213257 sec\n",
            "Time for epoch 46 is 10.8018217086792 sec\n",
            "Time for epoch 47 is 10.781848669052124 sec\n",
            "Time for epoch 48 is 10.77130126953125 sec\n",
            "Time for epoch 49 is 10.782172203063965 sec\n",
            "Time for epoch 50 is 10.774786233901978 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu1_KX8tm311"
      },
      "source": [
        "### What has the generator learned?\n",
        "\n",
        "The generator has learned how to take random input and produce images that look like MNIST digits.\n",
        "\n",
        "We give it a set of random vectors of size `NOISE_DIM=100` and call the generator with `training=False` to get our generated images.\n",
        "\n",
        "As you can see, the generated images kind of look like digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av9lz5QQXaBd",
        "outputId": "b0965c4d-5eeb-40a7-ae11-26ba3c669239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "num_examples_to_generate = 8\n",
        "seeds = tf.random.normal([num_examples_to_generate, NOISE_DIM])\n",
        "preds = generator(seeds, training=False)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(preds.shape[0]):\n",
        "  plt.subplot(1, 8, i+1)\n",
        "  plt.imshow(preds[i, :, :, 0], cmap=\"gray\")\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "_ = plt.show()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIFNIV4Uv88N"
      },
      "source": [
        "## Distributed Training\n",
        "\n",
        "Keras (and Tensorflow) allows you to define `strategies` to do distributed training. We will cover training on multiple GPUs on a single machine.\n",
        "\n",
        "This is the most common use case. AWS provides various GPU instances with upto 16 GPUs (P2, P3, G3, G4).\n",
        "\n",
        "Main approaches for distributing training:\n",
        "  * Data Parallelism -- model trains using part of the data on each GPU.\n",
        "  * Model Parallelism -- sub-models trained separately on each GPU.\n",
        "\n",
        "Two requirements:\n",
        "* For fault tolerance, it is recommended to use `ModelCheckpoint` callback to save the model periodically.\n",
        "* Also distributed training needs data to be provided as `tf.data.Datasets`.\n",
        "\n",
        "You can also train across a cluster of GPU enabled machines -- [see this document for more details](https://keras.io/guides/distributed_training/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_RZNWFvyemA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(Xtrain, ytrain), (Xtest, ytest) = keras.datasets.mnist.load_data()\n",
        "\n",
        "Xtrain = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1).astype(np.float32)\n",
        "Xtest = Xtest.reshape(Xtest.shape[0], 28, 28, 1).astype(np.float32)\n",
        "\n",
        "Xtrain = (Xtrain - 127.5) / 127.5\n",
        "Xtest = (Xtest - 127.5) / 127.5\n",
        "\n",
        "ytrain = keras.utils.to_categorical(ytrain)\n",
        "ytest = keras.utils.to_categorical(ytest)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(Xtrain, ytrain, test_size=0.2, random_state=1)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(Xtrain)).batch(BATCH_SIZE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((Xtest, ytest)).batch(BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm5j0GFIzWNA"
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",\n",
        "                                input_shape=(28, 28, 1)))\n",
        "  model.add(keras.layers.GlobalMaxPooling2D())\n",
        "  model.add(keras.layers.Dense(10))\n",
        "  return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT9EtYp_wmnl"
      },
      "source": [
        "### Data Parallelism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZicg1bQbUkt",
        "outputId": "13b162d5-e775-461e-e82c-bcdaf12971dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_ckpt = keras.callbacks.ModelCheckpoint(\"/tmp/model_1\", save_best_only=True)\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "with strategy.scope():\n",
        "  model = build_model()\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCgi3CiNdzHe",
        "outputId": "5f663c91-c6a7-40c0-ab3e-e0b125343033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "%%time\n",
        "model.fit(train_ds, epochs=5, validation_data=val_ds, callbacks=[model_ckpt])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "184/188 [============================>.] - ETA: 0s - loss: 9.1736 - accuracy: 0.0960INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /tmp/model_1/assets\n",
            "188/188 [==============================] - 3s 15ms/step - loss: 9.1864 - accuracy: 0.0962 - val_loss: 9.5849 - val_accuracy: 0.0988\n",
            "Epoch 2/5\n",
            "186/188 [============================>.] - ETA: 0s - loss: 9.5397 - accuracy: 0.0988INFO:tensorflow:Assets written to: /tmp/model_1/assets\n",
            "188/188 [==============================] - 2s 8ms/step - loss: 9.5409 - accuracy: 0.0988 - val_loss: 9.5836 - val_accuracy: 0.0988\n",
            "Epoch 3/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 9.5396 - accuracy: 0.0988 - val_loss: 9.5836 - val_accuracy: 0.0988\n",
            "Epoch 4/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 9.5406 - accuracy: 0.0988 - val_loss: 9.5836 - val_accuracy: 0.0988\n",
            "Epoch 5/5\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 9.5406 - accuracy: 0.0988 - val_loss: 9.5836 - val_accuracy: 0.0988\n",
            "CPU times: user 9.89 s, sys: 784 ms, total: 10.7 s\n",
            "Wall time: 10.8 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f958600ecf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cz9AuRJ2DCn"
      },
      "source": [
        "### Model Parallelism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKQ7vid2xSwt"
      },
      "source": [
        "input_a = keras.Input(shape=(140, 256))\n",
        "input_b = keras.Input(shape=(140, 256))\n",
        "\n",
        "shared_lstm = keras.layers.LSTM(64)\n",
        "\n",
        "# Process the first sequence on one GPU\n",
        "with tf.device('/gpu:0'):\n",
        "    encoded_a = shared_lstm(input_a)\n",
        "# Process the next sequence on another GPU\n",
        "with tf.device('/gpu:1'):\n",
        "    encoded_b = shared_lstm(input_b)\n",
        "\n",
        "# Concatenate results on CPU\n",
        "with tf.device('/cpu:0'):\n",
        "    merged_vector = keras.layers.concatenate(\n",
        "        [encoded_a, encoded_b], axis=-1)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIr1tUF-bxWn"
      },
      "source": [
        "### Using TPU\n",
        "\n",
        "__NOTE: Please set your Runtime to TPU for this part of the notebook.__\n",
        "\n",
        "TPUs are available on Colab and can make your training go faster. Using TPUs is similar to using GPUs with Data Parallelism from a programming point of view.\n",
        "\n",
        "You create a strategy, and then define and compile the model with the strategy scope.\n",
        "\n",
        "Code is adapted from this [Keras example of Pneumonia X-ray Image Classification with TPU](https://keras.io/examples/vision/xray_classification_with_tpus/).\n",
        "\n",
        "__NOTE:__ In this example, training on TPU is actually slower than training on GPU. This is because the model is small and more time is spent on TPU overhead than gained during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5aP8atFgb6K"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_5YeWLHch7g",
        "outputId": "29a810d7-3617-4857-cf35-781e41c93731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(\"Device:\", tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: grpc://10.124.125.58:8470\n",
            "WARNING:tensorflow:TPU system grpc://10.124.125.58:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.124.125.58:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.124.125.58:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.124.125.58:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of replicas: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcEhcLVjhCve"
      },
      "source": [
        "# rebuilding the train_ds dataset because we are changing runtimes\n",
        "(Xtrain, ytrain), (Xtest, ytest) = keras.datasets.mnist.load_data()\n",
        "\n",
        "Xtrain = Xtrain.reshape(Xtrain.shape[0], 28, 28, 1).astype(np.float32)\n",
        "Xtest = Xtest.reshape(Xtest.shape[0], 28, 28, 1).astype(np.float32)\n",
        "\n",
        "Xtrain = (Xtrain - 127.5) / 127.5\n",
        "Xtest = (Xtest - 127.5) / 127.5\n",
        "\n",
        "ytrain = keras.utils.to_categorical(ytrain)\n",
        "ytest = keras.utils.to_categorical(ytest)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(Xtrain, ytrain, test_size=0.2, random_state=1)\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(Xtrain)).batch(BATCH_SIZE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMzGWLkglMC"
      },
      "source": [
        "# same as build_model above, but need to redefine since we are changing runtimes\n",
        "def build_model_tpu():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\",\n",
        "                                input_shape=(28, 28, 1)))\n",
        "  model.add(keras.layers.GlobalMaxPooling2D())\n",
        "  model.add(keras.layers.Dense(10))\n",
        "  return model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYu3brb7d9EN"
      },
      "source": [
        "with strategy.scope():\n",
        "  model = build_model_tpu()\n",
        "  model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hMdQ49a7XIP",
        "outputId": "548ba1c0-c969-4568-c700-084cfa5e378e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "%%time\n",
        "model.fit(train_ds, epochs=5, validation_data=val_ds)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  1/188 [..............................] - ETA: 2:37 - loss: 8.8377 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0142s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0142s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "188/188 [==============================] - ETA: 0s - loss: 8.2153 - accuracy: 0.0980WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0116s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0116s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "188/188 [==============================] - 7s 36ms/step - loss: 8.2153 - accuracy: 0.0980 - val_loss: 8.1665 - val_accuracy: 0.1042\n",
            "Epoch 2/5\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 8.1876 - accuracy: 0.1020 - val_loss: 8.2229 - val_accuracy: 0.1047\n",
            "Epoch 3/5\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 8.1920 - accuracy: 0.1026 - val_loss: 8.1960 - val_accuracy: 0.1058\n",
            "Epoch 4/5\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 8.2840 - accuracy: 0.1027 - val_loss: 8.2001 - val_accuracy: 0.1064\n",
            "Epoch 5/5\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 8.2239 - accuracy: 0.1031 - val_loss: 8.2001 - val_accuracy: 0.1064\n",
            "CPU times: user 13.5 s, sys: 1.88 s, total: 15.4 s\n",
            "Wall time: 29.4 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f95d9e01390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DamGiEHVekhw"
      },
      "source": [
        "__NOTE: reset your runtime type to GPU for the remainder of the notebook, and for the next time you run this notebook.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvm2BjRufIwW"
      },
      "source": [
        "## Doing more with Keras\n",
        "\n",
        "Two aspects:\n",
        "* where to get more information about Keras?\n",
        "* how to keep up-to-date with Keras?\n",
        "\n",
        "### More Info about Keras\n",
        "\n",
        "We've tried to cram all of Keras within 3 hours in this tutorial.\n",
        "\n",
        "As a result, we have not only skipped over most of Deep Learning theory that Keras is built on, but also not covered many Keras classes that are targeted to specific situations.\n",
        "\n",
        "If you want to explore Keras further, check out the [Keras web site](https://keras.io/). It has tons of information, and is well-designed, like Keras. Here you will find:\n",
        "\n",
        "* Two learning tracks, one [for ML engineers](https://keras.io/getting_started/intro_to_keras_for_engineers) and one [for ML researchers](https://keras.io/getting_started/intro_to_keras_for_researchers).\n",
        "* The [Keras API Docs](https://keras.io/api/). I prefer the layout of this one, compared to [tf.keras API docs](https://www.tensorflow.org/api_docs/python/tf/keras) but both should provide similar information, and as we move into the future, perhaps the latter will be more authoritative.\n",
        "* [Keras examples](https://keras.io/examples/) contributed by project members and users, useful for those who learn better from end-to-end examples.\n",
        "\n",
        "If you prefer books, here are some recommendations, from theory heavy to practice heavy.\n",
        "\n",
        "* [Deep Learning](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/) by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n",
        "* [Deep Learning with Python](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/) by Francois Chollet.\n",
        "* [Deep Learning with Tensorflow 2 and Keras](https://www.amazon.com/Deep-Learning-TensorFlow-Keras-Regression/dp/1838823417/) by Antonio Gulli, Amita Kapoor, and Sujit Pal.\n",
        "\n",
        "### Keeping up-to-date with Keras\n",
        "\n",
        "* Follow [Francois Chollet (@fchollet) on Twitter](https://twitter.com/fchollet) -- he posts about interesting additions to Keras as they happen, so its a good way to keep up-to-date on whats latest in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYdtx60uepqc"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}