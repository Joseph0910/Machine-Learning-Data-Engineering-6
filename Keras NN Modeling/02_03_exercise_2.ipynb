{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-03-exercise-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCeG1Rh4IBPqJZoDiH8J7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpal/keras-tutorial-osdc2020/blob/master/02_03_exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wMS3MvTiLfZ"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "In this exercise we will put together things that we have learned in this session. Our tasks are as follows:\n",
        "\n",
        "* Build a transfer learning based classifier for the [CIFAR-10 dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10) by combining the base layers of an [VGG-16 model trained on ImageNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16) and a 2 layer Dense network.\n",
        "  * Make sure the output of the VGG16 is followed by a BatchNorm layer, and the Dense layers have Dropouts.\n",
        "* Train the network with transfer learning.\n",
        "* Evaluate the performance of the model against the CIFAR-10 test set.\n",
        "* Use the [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to do data augmentation and retrain the network.\n",
        "* Evaluate the performance again.\n",
        "* Use the Learning Rate Finder to determine a good range of learning rates for the model (with data augmentation).\n",
        "* Use a Cyclic Learning Rate (CLR) schedule to retrain the augmented transfer learning model.\n",
        "* Evaluate the performance again.\n",
        "* Present your findings.\n",
        "\n",
        "__NOTE__: Make sure you have your runtime set to GPU for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsLCCnNcmcbH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUm7odA5l4Im"
      },
      "source": [
        "## Get the data\n",
        "\n",
        "We have already seen the CIFAR-10 dataset. From [it's website](https://www.cs.toronto.edu/~kriz/cifar.html), it is a collection of (32, 32, 3) color images, with 50,000 images in its training set and 10,000 images in its test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtgwUrK5o5Fx"
      },
      "source": [
        "CIFAR10_CLASSES = [\n",
        "  \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "  \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5BdG8v5h_-0"
      },
      "source": [
        "# TODO: get the data from Keras built-in datasets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8iQLnzmxq5"
      },
      "source": [
        "# TODO: convert training and test labels to categorical (one-hot) format"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BOLRpF-pX3n"
      },
      "source": [
        "## Task 1: Build Transfer Learning Classifer\n",
        "\n",
        "Download the pre-trained InceptionV3 network from `keras.applications`, set it up as a layer, and add in your 3-layer Dense network.\n",
        "\n",
        "Remember to wrap the model in a `build_model` function, as you will be re-using the code to do this in later tasks as well.\n",
        "\n",
        "Compile the model using the Adam optimizer, Categorical Cross Entropy loss, and Accuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY-azvuApRjp"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "\n",
        "# TODO: download pretrained Inception V3 network (include_top=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WZOkFH9qJLq"
      },
      "source": [
        "# TODO: preprocess the training and test images using the InceptionV3 means"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHvVXV9zraCz"
      },
      "source": [
        "def build_model():\n",
        "  pass\n",
        "\n",
        "# TODO: set up the model, remember to wrap this in a build_model() function.\n",
        "# model_t1 = ..."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqdOzZKlreqS"
      },
      "source": [
        "# TODO: freeze the base model layers, i.e. make them non-trainable\n",
        "#       consider lifting this code block into your build_model function."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MpSGyBsr3bF"
      },
      "source": [
        "# TODO: compile the model with Adam optimizer, Categorical Cross Entropy loss\n",
        "#       and Accuracy metric."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqCvsS80suSm"
      },
      "source": [
        "## Task 2: Train Network\n",
        "\n",
        "Train the network for 50 epochs, using a batch size of 64. Use 20% of the training data for inline validation during the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcN1uF5HsSJ3"
      },
      "source": [
        "# TODO: train network, save history for later"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v8AcO73tFWL"
      },
      "source": [
        "def plot_training_curves(history):\n",
        "  plt.figure(figsize=(10, 5))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history[\"loss\"], label=\"train\")\n",
        "  plt.plot(history.history[\"val_loss\"], label=\"validation\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.legend(loc=\"best\")\n",
        "\n",
        "  plt.subplot(1, 2, 2)  \n",
        "  plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
        "  plt.plot(history.history[\"val_accuracy\"], label=\"validation\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.legend(loc=\"best\")\n",
        "\n",
        "\n",
        "  plt.tight_layout()\n",
        "  _ = plt.show()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te7fMxcF1wr4"
      },
      "source": [
        "# TODO: plot the loss and accuracy plots for your training"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttji0poA2WmD"
      },
      "source": [
        "## Task 3: Evaluate Network\n",
        "\n",
        "Use the `model.evaluate()` function against the test set. First element in the returned array is the test loss, and second element is the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_pI5UWd2cBr"
      },
      "source": [
        "# TODO: evaluate the network you just trained"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-twdZocy2u7s"
      },
      "source": [
        "## Task 4: Augment Data and Retrain\n",
        "\n",
        "Data Augmentor should have the following parameters at minimum, plus any you feel is relevant to your dataset. See [ImageDataGenerator docs](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) for full list of possible operations.\n",
        "\n",
        "* rotation_range = 30\n",
        "* width_shift_range = 0.2\n",
        "* height_shift_range = 0.2\n",
        "* horizontal_flip = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAcml5vQ2mHk"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# TODO: declare an image data generator that allows the operations listed"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2jwn_iI889Z"
      },
      "source": [
        "# TODO: define model (again, reuse by calling the build_model function you defined earlier)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYUDyVnxbcJO"
      },
      "source": [
        "# TODO: compile this new model (call this model_t2 to differentiate from previous model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysa3P8wO98p2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO: split the training dataset 80/20 for training vs inline validation. This is \n",
        "#       because you don't want to get augmented versions of the validation set."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKFBNWGp9Lg8"
      },
      "source": [
        "# TODO: fit the model using the ImageDataGenerator, using batch size of 64 and\n",
        "#       10 epochs of training."
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22eirJIh-jNo"
      },
      "source": [
        "# TODO: plot the loss and accuracy plots for your training"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3jr6zQw_6zm"
      },
      "source": [
        "## Task 5: Evaluate Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0fZvNK5_y9u"
      },
      "source": [
        "# TODO: evaluate the network you just trained"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyauCk2WOJh_"
      },
      "source": [
        "## Task 6: LR Finder\n",
        "\n",
        "We want to find a good range for Learning Rates to use with our Cyclic Learning Rate schedule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s3w9_neAB83"
      },
      "source": [
        "import math\n",
        "from keras.callbacks import LambdaCallback\n",
        "import numpy as np\n",
        "\n",
        "class LRFinder:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # Log the learning rate\n",
        "        # lr = K.get_value(self.model.optimizer.lr)\n",
        "        lr = keras.backend.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # Log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Check whether the loss got too large or NaN\n",
        "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.best_loss = loss\n",
        "\n",
        "        # Increase the learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "        # K.set_value(self.model.optimizer.lr, lr)\n",
        "        keras.backend.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1, **kw_fit):\n",
        "        # If x_train contains data for multiple inputs, use length of the first input.\n",
        "        # Assumption: the first element in the list is single input; NOT a list of inputs.\n",
        "        N = x_train[0].shape[0] if isinstance(x_train, list) else x_train.shape[0]\n",
        "\n",
        "        # Compute number of batches and LR multiplier\n",
        "        num_batches = epochs * N / batch_size\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
        "        # Save weights into a file\n",
        "        initial_weights = self.model.get_weights()\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        # original_lr = K.get_value(self.model.optimizer.lr)\n",
        "        original_lr = keras.backend.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        # K.set_value(self.model.optimizer.lr, start_lr)\n",
        "        keras.backend.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(x_train, y_train,\n",
        "                       batch_size=batch_size, epochs=epochs,\n",
        "                       callbacks=[callback],\n",
        "                       **kw_fit)\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.set_weights(initial_weights)\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        # K.set_value(self.model.optimizer.lr, original_lr)\n",
        "        keras.backend.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def find_generator(self, generator, start_lr, end_lr, epochs=1, steps_per_epoch=None, **kw_fit):\n",
        "        if steps_per_epoch is None:\n",
        "            try:\n",
        "                steps_per_epoch = len(generator)\n",
        "            except (ValueError, NotImplementedError) as e:\n",
        "                raise e('`steps_per_epoch=None` is only valid for a'\n",
        "                        ' generator based on the '\n",
        "                        '`keras.utils.Sequence`'\n",
        "                        ' class. Please specify `steps_per_epoch` '\n",
        "                        'or use the `keras.utils.Sequence` class.')\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(epochs * steps_per_epoch))\n",
        "\n",
        "        # Save weights into a file\n",
        "        initial_weights = self.model.get_weights()\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        # original_lr = K.get_value(self.model.optimizer.lr)\n",
        "        original_lr = keras.backend.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        # K.set_value(self.model.optimizer.lr, start_lr)\n",
        "        keras.backend.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch,\n",
        "                                                      logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit_generator(generator=generator,\n",
        "                                 epochs=epochs,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 callbacks=[callback],\n",
        "                                 **kw_fit)\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.set_weights(initial_weights)\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        # K.set_value(self.model.optimizer.lr, original_lr)\n",
        "        keras.backend.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, x_scale='log'):\n",
        "        \"\"\"\n",
        "        Plots the loss.\n",
        "        Parameters:\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale(x_scale)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
        "        \"\"\"\n",
        "        Plots rate of change of the loss function.\n",
        "        Parameters:\n",
        "            sma - number of batches for simple moving average to smooth out the curve.\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "            y_lim - limits for the y axis.\n",
        "        \"\"\"\n",
        "        derivatives = self.get_derivatives(sma)[n_skip_beginning:-n_skip_end]\n",
        "        lrs = self.lrs[n_skip_beginning:-n_skip_end]\n",
        "        plt.ylabel(\"rate of loss change\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(lrs, derivatives)\n",
        "        plt.xscale('log')\n",
        "        plt.ylim(y_lim)\n",
        "        plt.show()\n",
        "\n",
        "    def get_derivatives(self, sma):\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
        "        return derivatives\n",
        "\n",
        "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
        "        derivatives = self.get_derivatives(sma)\n",
        "        best_der_idx = np.argmin(derivatives[n_skip_beginning:-n_skip_end])\n",
        "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLW0oBtKOzjm"
      },
      "source": [
        "# TODO: instantiate model and compile with optimizer=\"sgd\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zomsOu59TPYM"
      },
      "source": [
        "# TODO: instantiate the LRFinder and call find on it using a broad range of LR\n",
        "#       and 5 epochs of training"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK21OZuxPCjV"
      },
      "source": [
        "# TODO: plot the loss for LRFinder (call plot_loss())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tirfWvvxPECW"
      },
      "source": [
        "# TODO: plot the change of loss for LRFinder (call plot_loss_change())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEyWV7YxQ-_-"
      },
      "source": [
        "Based on plots above, reasonable values for Learning Rate for CLR seem to be between 1e-3 and 1e-1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hMM_Yb9RO8N"
      },
      "source": [
        "## Task 7: Train with CLR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNvKDN_rQctl"
      },
      "source": [
        "%%bash\n",
        "(\n",
        "if [ ! -d \"CLR\" ]; then\n",
        "  git clone https://github.com/bckenstler/CLR\n",
        "fi\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP894KlhRzTG"
      },
      "source": [
        "from CLR.clr_callback import CyclicLR\n",
        "\n",
        "# batch_size = 128\n",
        "# step_size = 4 * Xtrain.shape[0] // batch_size\n",
        "# clr = CyclicLR(mode=\"triangular2\",\n",
        "#                base_lr=1e-3,\n",
        "#                max_lr=1e-1,\n",
        "#                step_size=step_size)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBlEApGWR4XD"
      },
      "source": [
        "# TODO: instantiate a model, compile with SGD optimizer, categorical cross entropy loss,\n",
        "#       and accuracy metric, then train it for 10 epochs with batch size 64 and validation\n",
        "#       split of 20%. Remember to set the CLR as your learning rate scheduler callback.\n",
        "#       Also remember that you need to use data augmentation."
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Nm8hBSS2r5"
      },
      "source": [
        "# TODO: plot the loss and accuracy plots for your training"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4MeLkHsUKVD"
      },
      "source": [
        "## Task 8: Evaluate CLR based training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcMn_J9bUHha"
      },
      "source": [
        "# TODO: evaluate the network you just trained"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaKhspC3UbQI"
      },
      "source": [
        "## Task 9: Summarize your findings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUKdg08yUYnU"
      },
      "source": [
        "def plot_accuracies(methods, histories):\n",
        "  train_accs, val_accs = [], []\n",
        "  for method, history in zip(methods, histories):\n",
        "    train_accs.append(history.history[\"accuracy\"][-1])\n",
        "    val_accs.append(history.history[\"val_accuracy\"][-1])\n",
        "\n",
        "  xs = np.arange(len(methods))\n",
        "  width = 0.35\n",
        "  plt.bar(xs, train_accs, width, color=\"b\", label=\"train\")\n",
        "  plt.bar(xs+width, val_accs, width, color=\"orange\", label=\"val\")\n",
        "  plt.axhline(y=train_accs[0], linestyle=\"--\", color=\"b\", xmin=0, xmax=1)\n",
        "  plt.axhline(y=val_accs[0], linestyle=\"--\", color=\"orange\", xmin=0, xmax=1)\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.xticks(xs, methods, rotation=90)\n",
        "  plt.legend(loc=\"best\")\n",
        "  _ = plt.show()\n",
        "\n",
        "def list_scores(methods, scores):\n",
        "  for method, score in zip(methods, scores):\n",
        "    print(\"{:s}: {:.5f}\".format(method, score[1]))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUylKTDRW2LV"
      },
      "source": [
        "tasks = [\"vgg16\", \"vgg16+data\", \"vgg16+data+clr\"]\n",
        "\n",
        "# TODO: create a list of history and score objects similar to tasks above"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LtrIn-LXGRB"
      },
      "source": [
        "# TODO: plot the training and validation accuracies for each training;\n",
        "#       call plot_accuracies()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ft90EteXbLq"
      },
      "source": [
        "# TODO: list final test set evaluation scores for each trained model;\n",
        "#       call list_scores()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0lAmSfFXzn0"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}